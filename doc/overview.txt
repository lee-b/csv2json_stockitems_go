Overview
========

The main components here are:

   + src/lbgotest.com/lb/stockdatalib:

      This Go library provides:

         - An internal representation of StockItem objects, and associate
           datatypes such as Price values.
         - Functions for reading them from csv.Reader objects
         - MarshalJSON implementations for some types, such as Prices, to
           format them as required in JSON output.

      This library supports a streaming API, meaning that you can load one
      item at a time, work with it, and write it out.  All data does NOT
      need to be kept in memory at once.

   + csv_items_to_json:

      This command-line tool, a thin wrapper over the above library, reads
      stock items from a specifically formatted (as specified) CSV file, and
      writes them to a JSON file.  Both filenames are taken on the command
      line.


Goals
=====

* Import stock item data from CSV format

* Export stock item data to JSON format

* Extensible code (as per requirements), which I take to mean:
  - re-usable input parsing -- parsing to a usable internal data format
  - re-usable output -- output from a usable internal data format

* "Production-ready", which I take to mean:
  - sane, usable, safe, pre-parsed internal data format:
      + nil, rather than the string value "nil"
      + proper numeric values rather than unparsed string values with or
        without currency symbols
      + fixed-precision or integer format currency values, rather than
        floating point values, which may have precision problems.
  - VALIDATED data, RATHER THAN BLIND / GENERIC CONVERSION (which would have
    been much easier, faster, and more reusable, in many ways, but also less
    useful in terms of reusable INTERNAL stock item data)

* Use go standard library facilities for actual csv/json I/O (to minimise new
  code, and assist with extensibility)

* Reusable code (partly to meet "extensible code", partly as best-practice)

* Precision control in currency values, to avoid floating point precision
  issues.

* Unit-tested

* Functionally tested on actual data files

* Verified against given scenario

* Verified on larger data set, proving streaming/big data should work

* "Production-ready", although this is a vague term without knowing
  requirements of the production environment in question.  The requirements
  I've assumed are:
    + Input validation / security
    + Big data (stream-based processing rather than loading all data into
      memory at once)
    + Reliability, as specified above in terms of tests, required features,
       etc.


Non-goals & assumptions
=======================

* First and foremost, this is taken to be a test of programming skill and
  professionalism, WITHIN the context of the company setting the test, and in
  the
  context of sales data manipulation.  It is NOT taken as a request to create
  an entirely generic data conversion tool.
  As such, I have opted to write code which performs proper stock item
  validation etc., rather than a "quick hack that simply sucks in data and
  spits it out again", so to speak, with little regard for the data being
  manipulated.
  The important implication is that, ultimately, "code reusability" in this
  context, should favour building useful internal sales data and frameworks
  around such data, rather than building generically reusable CSV / JSON
  tools.
  If SUCH generic tools were needed, they almost certainly already exist, and
  should be used off-the-shelf.  However, if they do not exist, they COULD be
  created. Covering all use cases with such a generic tool is assumed to be
  unhelpful and a wasteful distraction, for this particular use case, though.

* JSON does not have a keyword "nil", although it's supported as a string, but
  it DOES recognise the special keyword support "null".  This presents two
  options, neither of which is ideal, and so a choice had to be made:

      Choice 1:

          Ignore JSON's support for null, despite the overarching goal to
          produce a **JSON** file (as opposed to a file in a format SIMILAR
          to JSON.  Ignore Go's built-in support for marshalling structs,
          and implement a custom marshaller to output the non-compliant
          near-JSON format.  Note that this would also conflict with the goal
          of reusability, since it means having to implement custom
          JSON parsers for all output data, and might lock all future
          implementations / rewrites of this tool into creating similarly
          non-standard files.

      Choice 2:

          Assume that null was meant rather than nil in the example output
          file -- perhaps that nil was written because it is called this in
          Go, rather than JSON.  Assume that the intention is to create
          STANDARD, compatible, JSON, following established internet
          RFCs.  This would also assist with the goal of reusable code.
     
  Rather than blindly break a standard format, which, in my experience, is
  the cause of many code maintence and incompatibility issues, I have assumed
  that choice 2 was intended, and would be preferred here.

  If choice 1 was, in fact, intended, then note that this would have been
  very easy to accomplish, by implementing a MarshalJSON() method for the
  StockItem class, and adding JSON-formatted bytes manually for the overall
  object, along with calling Marshal() to obtain the bytes for subfields.
  Something similar IS done in the top-level command-line tool, to wrap the
  individual StockItems in JSON list.

  Of course, in real, day-to-day work, it would be easy to check this with
  whoever wrote the format specification.

* The sample data, provided as a PDF from Word, contained soft-hypens in place
  of ASCII minus signs.  I've assumed ASCII minus signs were intended.

* Very specific output examples are given, in a particular variant of JSON
  which uses unquoted, single-word strings, for example.  This is assumed to
  be an unimportant, "over-specified", detail, as the ultimate goal, as
  specified, seems to to be creation of JSON-compatible data.

* From the sample data, it appears that stock items have at least 5 fields (up
  to pruce_type).  Since no defaults are provided in the task specification
  for price_type, quantity, etc., it's unclear how this should be populated
  otherwise.  So, this is assumed, and items with less fields will NOT parse
  with this code, as-is.  Defaults could be easily added if needed, though.

* The input data's "cost" field varies between beginning with a dollar sign,
  and not having a dollar sign.  It's assumed that all prices may have or not
  have dollar signs, and that, if they don't, they are parsed the same way (as
  decimal dollar values), otherwise: i.e., that "80" == "$80", "80.10" ==
  "$80.10", "80.1" == "$80.10", etc.

* Currency tracking and conversion is not a goal: input file OFTEN has
  currency symbols, but they are all the same (dollars) and the output doesn't
  seem to have a notion of currency, which suggests that this currency support
  is not needed for the purposes of the test.  Another interpretation would be
  that the intention is to convert the given input currency to local currency.
  However, input currency is NOT always given, and no data necessary for
  conversion (currency conversion rates, conversion data source, etc.) is
  given.

* "quantity_on_hand" field values are assumed to be integers, as I saw no
  examples of fractional values in this field.

* Prices are assumed to have integer cents values (i.e., no fractions of a
  cent), as this fits the data presented in examples, and logically, makes
  sense for most retail purposes.

* Perfectly idiomatic Go (preferred style, preferred design patterns, etc.) is
  not a goal. I do not know Go yet, and researching Go idioms extensively for
  all parts of the code could take too much time for the purposes of this
  test.
  Attempts at idiomatic code have been made where reasonable, though, such as
  using JSON Reader / Writer, using iota rather than simply typing values,
  etc.

* A serialisation-style interface to the input CSV parsing, mirroring the
  marshalling of JSON output, would be ideal, especially for extensibility.
  Only a token version of this has been implemented (StockItem.Unmarshall), as
  it's not clear how future extensibility should work at present.

* I've assumed that "modifier" entries (which are paired fields:
  modifier_1_name, and modifier_1_price, for example) can either be specified,
  or not specified, but that one field should not be present without the
  other. If this is detected during input parsing, then an error is reported.


Future improvements / wishlist
==============================

* Multiple currencies (swap out Cents for a Monetary type)
* Add input & output file buffering for improved performance
* Use Go channels to create parallel input / parsing / output, for better
  overall throughput.  To do this without a genuine need may be premature
  optimisation, however.


